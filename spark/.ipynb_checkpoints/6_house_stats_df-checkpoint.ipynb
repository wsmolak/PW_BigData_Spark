{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/logo.png)\n",
    "\n",
    "# Przetwarzanie Big Data z użyciem Apache Spark\n",
    "\n",
    "Autor notebooka: Jakub Nowacki.\n",
    "\n",
    "\n",
    "## Spark SQL - statystyki z użyciem metod DataFrame \n",
    "\n",
    "W katalogu `data/` znajduje się plik `rollingsales_bronx.csv`.\n",
    "\n",
    "Poniżej analizujemy dane statystyczne domów: liczność, średnie powierzchnie, lata i ceny, grupując po dzielnicy i typie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as func\n",
    "import pyspark.sql.types as types\n",
    "sc = pyspark.SparkContext(appName='HouseStatsDF')\n",
    "sqlContext = pyspark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Użycie Row to jedna z metod tworzenia DataFrame z danych; nie wymaga dodatkowego schematu\n",
    "# zobacz dokumentację: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
    "def from_csv(line):\n",
    "    # Stałe podane są tylko dla czytelności\n",
    "    HOOD_COLUMN = 1\n",
    "    TYPE_COLUMN = 2;\n",
    "    LAND_AREA_COLUMN = 14;\n",
    "    GROSS_AREA_COLUMN = 15;\n",
    "    YEAR_COLUMN = 16;\n",
    "    PRICE_COLUMN = 19;\n",
    "    # Dzielimy linię na kolumny i przypisujemy do kluczy w słowniku\n",
    "    c = line.split(',')\n",
    "    row = dict()\n",
    "    row['hood'] = c[HOOD_COLUMN];\n",
    "    row['type'] = c[TYPE_COLUMN];\n",
    "    row['landArea'] = int(re.sub(r'[^\\d]', '', c[LAND_AREA_COLUMN]));\n",
    "    row['grossArea'] = int(re.sub(r'[^\\d]', '', c[GROSS_AREA_COLUMN]));\n",
    "    row['year'] = int(re.sub(r'[^\\d]', '', c[YEAR_COLUMN]));\n",
    "    row['price'] = int(re.sub(r'[^\\d]', '', c[PRICE_COLUMN]));\n",
    "    # Zwracamy obiekt Row\n",
    "    return pyspark.Row(**row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Używamy powyższą metodę generującą obiekty Row do przeczytania CSV linia po linii\n",
    "# Zauważ, że tworzymy zwykłe RDD\n",
    "sales_rdd = sc.textFile('data/rollingsales_bronx.csv').map(lambda line: from_csv(line))\n",
    "# Z RDD złożonego z obiektów Row możemy bezpośrednio stworzyć DataFrame bez dodatkowego schematu\n",
    "sales = sqlContext.createDataFrame(sales_rdd)\n",
    "sales.printSchema()\n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a1 = sales.groupBy('hood', 'type') \\\n",
    "    .agg(func.count('type').alias('count'),\n",
    "         func.avg('landArea').alias('avgLandArea'),\n",
    "         func.avg('year').alias('avgYear'),\n",
    "         func.avg('price').alias('avgPrice')) \\\n",
    "    .orderBy('hood', 'type')\n",
    "a1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadania\n",
    "\n",
    "* Popraw wyniki usuwając błędne dane.\n",
    "* Zastosuj odpowiednią prezentację dany w zależności od typu.\n",
    "* Policz średnie dla domów z XX w. tylko dla grup zawierających więcej niż 10 domów.\n",
    "* ★ Policz średnie tylko dla 10 najbogatszych dzielnic."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
